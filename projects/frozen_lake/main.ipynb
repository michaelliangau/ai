{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map action to direction\n",
    "action_to_direction = {\n",
    "    0: 'UP',\n",
    "    1: 'RIGHT',\n",
    "    2: 'DOWN',\n",
    "    3: 'LEFT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'F' 'F' 'H']\n",
      " ['H' 'F' 'H' 'H']\n",
      " ['F' 'F' 'F' 'F']\n",
      " ['F' 'F' 'F' 'G']]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the environment\n",
    "class GridWorld:\n",
    "    def __init__(self, grid_size=4, hole_count=4):\n",
    "        \"\"\"Initialize the environment.\n",
    "\n",
    "        This is a static grid world environment.\n",
    "        \n",
    "        Args:\n",
    "            grid_size (int): size of the grid (default is 4)\n",
    "            hole_count (int): number of holes in the grid (default is 4)\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.state_space = np.arange(grid_size * grid_size)\n",
    "        self.action_space = np.arange(4) # 0: Up, 1: Right, 2: Down, 3: Left\n",
    "\n",
    "        # Initialize the grid\n",
    "        self.hole_count = hole_count\n",
    "        self.state = 0\n",
    "        self.grid = np.full((self.grid_size, self.grid_size), 'F') # Fill the grid with frozen blocks\n",
    "        self.grid[0, 0] = 'S' # Place the start block\n",
    "        self.grid[-1, -1] = 'G' # Place the goal block\n",
    "\n",
    "        # Place the hole blocks randomly\n",
    "        np.random.seed(0)  # For reproducibility\n",
    "        for _ in range(self.hole_count):\n",
    "            row, col = np.random.randint(self.grid_size, size=2)\n",
    "            # Ensure we don't place a hole at the start or goal\n",
    "            while (row, col) in [(0, 0), (self.grid_size - 1, self.grid_size - 1)]:\n",
    "                row, col = np.random.randint(self.grid_size, size=2)\n",
    "            self.grid[row, col] = 'H'\n",
    "\n",
    "    def _get_grid_position(self):\n",
    "        \"\"\"Converts a state number to a position in a 4x4 grid.\n",
    "\n",
    "        Args:\n",
    "            state (int): The state number, between 0 and 15 inclusive.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The grid position as (row, column).\n",
    "        \"\"\"\n",
    "        row = self.state // self.grid_size\n",
    "        col = self.state % self.grid_size\n",
    "        return (row, col)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state\n",
    "\n",
    "        The grid stays the same, only the agent's state is reset to the start block.\n",
    "\n",
    "        Returns:\n",
    "            state (int): initial state\n",
    "        \"\"\"\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    # Step 3: Take an action and return the next state and reward\n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return the next state and reward\n",
    "\n",
    "        Args:\n",
    "            action (int): action to take\n",
    "\n",
    "        Returns:\n",
    "            state (int): next state\n",
    "            reward (int): reward\n",
    "        \"\"\"\n",
    "        row, col = self._get_grid_position()\n",
    "        if action == 0:  # Up\n",
    "            row = max(row - 1, 0)\n",
    "        elif action == 1:  # Right\n",
    "            col = min(col + 1, self.grid_size - 1)\n",
    "        elif action == 2:  # Down\n",
    "            row = min(row + 1, self.grid_size - 1)\n",
    "        elif action == 3:  # Left\n",
    "            col = max(col - 1, 0)\n",
    "            \n",
    "        \n",
    "        # Update the state\n",
    "        next_state = row * self.grid_size + col\n",
    "        self.state = next_state\n",
    "        \n",
    "        # Get reward based on current state\n",
    "        reward = self._get_reward()\n",
    "\n",
    "        return next_state, reward\n",
    "\n",
    "    def _get_reward(self):\n",
    "        \"\"\"Return the reward based on the current state.\n",
    "        \n",
    "        Returns:\n",
    "            reward (int): -1 for falling into a hole, 1 for reaching the goal, and 0 otherwise\n",
    "        \"\"\"\n",
    "        row, col = self._get_grid_position()\n",
    "        if self.grid[row, col] == 'H':  # If the agent falls into a hole\n",
    "            return -1\n",
    "        elif self.grid[row, col] == 'G':  # If the agent reaches the goal\n",
    "            return 1\n",
    "        else:  # If the agent is on a frozen block\n",
    "            return 0\n",
    "\n",
    "env = GridWorld()\n",
    "env.reset()\n",
    "print(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define a simple agent\n",
    "class SimpleAgent:\n",
    "    def __init__(self, num_states=16, num_actions=4, alpha=0.9, gamma=0.95, epsilon=0.5):\n",
    "        \"\"\"Init the agent.\n",
    "        \n",
    "        Args:\n",
    "            num_states (int): number of states. Defaults to 16 (4 x 4 grid).\n",
    "            num_actions (int): number of actions. Defaults to 4 (left, down, right, up).\n",
    "            alpha (float, optional): learning rate. Defaults to 0.5.\n",
    "            gamma (float, optional): discount factor. Defaults to 0.95.\n",
    "            epsilon (float, optional): exploration rate. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Q table: Expected return (not reward) for each state-action pair \n",
    "        self.Q = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    def get_epsilon_greedy_action(self, state):\n",
    "        \"\"\"Pick a random action with probability epsilon, otherwise pick the best action\n",
    "        \n",
    "        Args:\n",
    "            state (int): current state\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.num_actions) # explore\n",
    "        else:\n",
    "            # exploit\n",
    "            # find the indices of the maximum values\n",
    "            max_indices = np.where(self.Q[state] == np.amax(self.Q[state]))[0]\n",
    "            # choose randomly from those indices\n",
    "            return np.random.choice(max_indices)\n",
    "\n",
    "    \n",
    "    def update_Q(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q table.\n",
    "\n",
    "        The Q-table is the expected return for each state-action pair.\n",
    "\n",
    "        This is the core mechanics of Q-learning and implements the Bellman optimality\n",
    "        equation for Q-values. This is a subset of the general Bellman equation.\n",
    "        \n",
    "        Reference principle of optimality on why Q learning works. Optimal policy of\n",
    "        subsequences is also optimal for the original sequence.\n",
    "\n",
    "        Args:\n",
    "            state (int): current state\n",
    "            action (int): action taken\n",
    "            reward (int): reward received\n",
    "            next_state (int): next state\n",
    "        \"\"\"\n",
    "        # Calculate temporal difference target (TD target). Expected future returns based\n",
    "        # off a combination of received reward and the expected future returns from\n",
    "        # best action in the next state.\n",
    "        best_next_action = np.argmax(self.Q[next_state])\n",
    "        td_target = reward + self.gamma * self.Q[next_state][best_next_action]\n",
    "\n",
    "        # Calculate temporal difference error (TD error). Difference between the\n",
    "        # implied expected return from received reward and the current Q value prior to\n",
    "        # update.\n",
    "        td_error = td_target - self.Q[state][action]\n",
    "\n",
    "        # Update the Q table value, multiplying the learning rate (alpha) by the error.\n",
    "        self.Q[state][action] += self.alpha * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'F' 'F' 'H']\n",
      " ['H' 'F' 'H' 'H']\n",
      " ['F' 'F' 'F' 'F']\n",
      " ['F' 'F' 'F' 'G']]\n",
      "Episode 1000/10000 completed\n",
      "Episode 2000/10000 completed\n",
      "Episode 3000/10000 completed\n",
      "Episode 4000/10000 completed\n",
      "Episode 5000/10000 completed\n",
      "Episode 6000/10000 completed\n",
      "Episode 7000/10000 completed\n",
      "Episode 8000/10000 completed\n",
      "Episode 9000/10000 completed\n",
      "Episode 10000/10000 completed\n",
      "Training finished.\n",
      "Q table:\n",
      "[[ 0.73509189  0.77378094 -1.          0.73509189]\n",
      " [ 0.77378094  0.73509189  0.81450625  0.73509189]\n",
      " [ 0.73509189 -1.         -1.          0.77378094]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.77378094 -1.          0.857375   -1.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-1.          0.857375    0.857375    0.81450625]\n",
      " [ 0.81450625  0.9025      0.9025      0.81450625]\n",
      " [-1.          0.95        0.95        0.857375  ]\n",
      " [-1.          0.95        1.          0.9025    ]\n",
      " [ 0.81450625  0.9025      0.857375    0.857375  ]\n",
      " [ 0.857375    0.95        0.9025      0.857375  ]\n",
      " [ 0.9025      1.          0.95        0.9025    ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Training loop\n",
    "\n",
    "# Initialize agent and environment\n",
    "env = GridWorld(grid_size=4, hole_count=4)\n",
    "print(env.grid)\n",
    "agent = SimpleAgent(env.state_space.shape[0], env.action_space.shape[0])\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the state, env doesn't change\n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = agent.get_epsilon_greedy_action(state)\n",
    "        next_state, reward = env.step(action)\n",
    "        agent.update_Q(state, action, reward, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if reward == -1 or reward == 1:  # agent fell in a hole or reached the goal\n",
    "            break\n",
    "    \n",
    "    # Print out progress\n",
    "    if (episode + 1) % 1000 == 0:\n",
    "        print(f\"Episode {episode + 1}/{num_episodes} completed\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "print(\"Q table:\")\n",
    "print(agent.Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
