{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transducer PyTorch implementation\n",
    "Goal: In this notebook, we'll use it to insert missing vowels into a sentence.\n",
    "\n",
    "This is an image of the architecture.\n",
    "\n",
    "![RNN-T architecture](images/rnnt_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-06 08:21:22--  https://raw.githubusercontent.com/lorenlugosch/infer_missing_vowels/master/data/train/war_and_peace.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3196229 (3.0M) [text/plain]\n",
      "Saving to: ‘war_and_peace.txt.1’\n",
      "\n",
      "war_and_peace.txt.1 100%[===================>]   3.05M  4.39MB/s    in 0.7s    \n",
      "\n",
      "2023-05-06 08:21:24 (4.39 MB/s) - ‘war_and_peace.txt.1’ saved [3196229/3196229]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get training data.\n",
    "!wget https://raw.githubusercontent.com/lorenlugosch/infer_missing_vowels/master/data/train/war_and_peace.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import IPython\n",
    "import string\n",
    "\n",
    "\n",
    "# Variables\n",
    "NULL_INDEX = 0\n",
    "encoder_dim = 1024\n",
    "predictor_dim = 1024\n",
    "joiner_dim = 1024\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder network\n",
    "# The encoder is any network that can take as input a variable-length sequence: so, RNNs, CNNs, and self-attention/Transformer encoders will all work.\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder network.\n",
    "    \n",
    "    Input: Audio input\n",
    "\n",
    "    Output: Encoded speech features. Context-aware encodings.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(num_inputs, encoder_dim)\n",
    "        self.rnn = torch.nn.GRU(input_size=encoder_dim, hidden_size=encoder_dim, num_layers=3, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.linear = torch.nn.Linear(encoder_dim*2, joiner_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.embed(out)\n",
    "        out = self.rnn(out)[0]\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor network\n",
    "# The predictor is any causal network (= can't look at the future): in other words, unidirectional RNNs, causal convolutions, or masked self-attention.\n",
    "\n",
    "class Predictor(torch.nn.Module):\n",
    "    \"\"\"Predictor network.\n",
    "    \n",
    "    Input: Text inputs (labels).\n",
    "    \n",
    "    Output: RNN hidden states for each autoregressive timestep input.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_outputs):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(num_outputs, predictor_dim)\n",
    "        self.rnn = torch.nn.GRUCell(input_size=predictor_dim, hidden_size=predictor_dim)\n",
    "        self.linear = torch.nn.Linear(predictor_dim, joiner_dim)\n",
    "        \n",
    "        self.initial_state = torch.nn.Parameter(torch.randn(predictor_dim))\n",
    "        self.start_symbol = NULL_INDEX # In the original paper, a vector of 0s is used; just using the null index instead is easier when using an Embedding layer.\n",
    "\n",
    "    def forward_one_step(self, input, previous_state):\n",
    "        embedding = self.embed(input)\n",
    "        state = self.rnn.forward(embedding, previous_state)\n",
    "        out = self.linear(state)\n",
    "        return out, state\n",
    "\n",
    "    def forward(self, y):\n",
    "        batch_size = y.shape[0]\n",
    "        U = y.shape[1]\n",
    "        outs = []\n",
    "        state = torch.stack([self.initial_state] * batch_size).to(y.device)\n",
    "        for u in range(U+1): # need U+1 to get null output for final timestep \n",
    "            if u == 0:\n",
    "                decoder_input = torch.tensor([self.start_symbol] * batch_size).to(y.device)\n",
    "            else:\n",
    "                decoder_input = y[:,u-1]\n",
    "            out, state = self.forward_one_step(decoder_input, state)\n",
    "            outs.append(out)\n",
    "        out = torch.stack(outs, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joiner(torch.nn.Module):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(Joiner, self).__init__()\n",
    "        self.linear = torch.nn.Linear(joiner_dim, num_outputs)\n",
    "\n",
    "    def forward(self, encoder_out, predictor_out):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            encoder_out: Output of the encoder network. Shape [batch_size, T, 1, encoder_dim]. # TODO Confirm that these shapes are correct.\n",
    "            predictor_out: Output of the predictor network. Shape [batch_size, 1, U, predictor_dim]. # TODO Confirm that these shapes are correct.\n",
    "        \"\"\"\n",
    "        out = encoder_out + predictor_out\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alignment matrix\n",
    "\n",
    "![RNN-T alignment matrix](images/rnnt_alignment_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transducer(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Transducer, self).__init__()\n",
    "        self.encoder = Encoder(num_inputs)\n",
    "        self.predictor = Predictor(num_outputs)\n",
    "        self.joiner = Joiner(num_outputs)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda:0\"\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def compute_forward_prob(self, joiner_out, T, U, y):\n",
    "        \"\"\"Compute forward probability.\n",
    "\n",
    "        Args:\n",
    "            joiner_out: tensor of shape (B, T_max, U_max+1, #labels)\n",
    "            T: list of input lengths\n",
    "            U: list of output lengths \n",
    "            y: label tensor (B, U_max+1)\n",
    "\n",
    "        Returns:\n",
    "            log_probs: tensor of shape (B)\n",
    "        \"\"\"\n",
    "        # TODO understand this\n",
    "        B = joiner_out.shape[0]\n",
    "        T_max = joiner_out.shape[1]\n",
    "        U_max = joiner_out.shape[2] - 1\n",
    "        log_alpha = torch.zeros(B, T_max, U_max+1).to(self.device)\n",
    "        for t in range(T_max):\n",
    "            for u in range(U_max+1):\n",
    "                if u == 0:\n",
    "                    if t == 0:\n",
    "                        log_alpha[:, t, u] = 0.\n",
    "\n",
    "                    else: #t > 0\n",
    "                        log_alpha[:, t, u] = log_alpha[:, t-1, u] + joiner_out[:, t-1, 0, NULL_INDEX] \n",
    "                            \n",
    "                else: #u > 0\n",
    "                    if t == 0:\n",
    "                        log_alpha[:, t, u] = log_alpha[:, t,u-1] + torch.gather(joiner_out[:, t, u-1], dim=1, index=y[:,u-1].view(-1,1) ).reshape(-1)\n",
    "                    \n",
    "                    else: #t > 0\n",
    "                        log_alpha[:, t, u] = torch.logsumexp(torch.stack([\n",
    "                            log_alpha[:, t-1, u] + joiner_out[:, t-1, u, NULL_INDEX],\n",
    "                            log_alpha[:, t, u-1] + torch.gather(joiner_out[:, t, u-1], dim=1, index=y[:,u-1].view(-1,1) ).reshape(-1)\n",
    "                        ]), dim=0)\n",
    "\n",
    "        log_probs = []\n",
    "        for b in range(B):\n",
    "            log_prob = log_alpha[b, T[b]-1, U[b]] + joiner_out[b, T[b]-1, U[b], NULL_INDEX]\n",
    "            log_probs.append(log_prob)\n",
    "        log_probs = torch.stack(log_probs) \n",
    "        return log_prob\n",
    "\n",
    "    def compute_loss(self, x, y, T, U):\n",
    "        encoder_out = self.encoder.forward(x)\n",
    "        predictor_out = self.predictor.forward(y)\n",
    "        joiner_out = self.joiner.forward(encoder_out.unsqueeze(2), predictor_out.unsqueeze(1)).log_softmax(3)\n",
    "        loss = -self.compute_forward_prob(joiner_out, T, U, y).mean()\n",
    "        return loss\n",
    "  \n",
    "    def compute_single_alignment_prob(self, encoder_out, predictor_out, T, U, z, y):\n",
    "        \"\"\"Computes the probability of one alignment, z.\n",
    "\n",
    "        What does the encoder_out network represent?\n",
    "        What does the predictor_out network represent?\n",
    "\n",
    "\n",
    "        Args:\n",
    "            encoder_out: Output of the encoder network. Shape [T, encoder_dim]. T = 4. In this example it is [4, 1024]\n",
    "            predictor_out: Output of the predictor network. Shape [U, predictor_dim]. U = 3+1 = 4. In this example it is [4, 1024]\n",
    "            T: Length of the encoder_out sequence.\n",
    "            U: Length of the predictor_out sequence.\n",
    "            z: Alignment. List of 0s and 1s, where 0 means \"right\" (increment t, encoder output) and 1 means \"down\" (increment u, label).\n",
    "            y: Label sequence. List of labels.\n",
    "        \n",
    "        Returns:\n",
    "            logprob: Log probability of the alignment.\n",
    "        \"\"\"\n",
    "        t = 0; u = 0 # t and u are merely movements in the alignment matrix that we'll use to index into the encoder_out and predictor_out matrices.\n",
    "        t_u_indices = []\n",
    "        y_expanded = []\n",
    "        for step in z:\n",
    "            t_u_indices.append((t,u))\n",
    "            if step == 0: # right (null)\n",
    "                y_expanded.append(NULL_INDEX)\n",
    "                t += 1\n",
    "            if step == 1: # down (label)\n",
    "                y_expanded.append(y[u])\n",
    "                u += 1\n",
    "\n",
    "        t_u_indices.append((T-1,U))\n",
    "        y_expanded.append(NULL_INDEX)\n",
    "\n",
    "        t_indices = [t for (t,u) in t_u_indices]\n",
    "        u_indices = [u for (t,u) in t_u_indices]\n",
    "        encoder_out_expanded = encoder_out[t_indices] # Indexed into the encoder/predictor_out lists using the t_u alignments.\n",
    "        predictor_out_expanded = predictor_out[u_indices]\n",
    "        joiner_out = self.joiner.forward(encoder_out_expanded, predictor_out_expanded).log_softmax(1) # joiner network yoinks the alignments at each timestep ((0, 0) hidden states) through it and pops out softmaxed probabilities.\n",
    "        logprob = -torch.nn.functional.nll_loss(input=joiner_out, target=torch.tensor(y_expanded).long().to(self.device), reduction=\"sum\") # TODO HERE!!!!!! Why is the target expanded?\n",
    "        IPython.embed()\n",
    "        return logprob  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.16 (default, Dec  7 2022, 01:27:54) \n",
      "Type 'copyright', 'credits' or 'license' for more information\n",
      "IPython 8.7.0 -- An enhanced Interactive Python. Type '?' for help.\n",
      "\n",
      "Out[1]: [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2), (3, 3)]\n",
      "\n",
      "Out[2]: [0, 0, 0, tensor(3), tensor(1), tensor(20), 0]\n",
      "\n",
      "Out[3]: (0, 0, 0, 1, 1, 1)\n",
      "\n",
      "Out[4]: [0, 1, 2, 3, 3, 3, 3]\n",
      "\n",
      "Out[5]: [0, 0, 0, 0, 1, 2, 3]\n",
      "\n",
      "Out[6]: \n",
      "tensor([[-0.0871, -0.2618, -1.1075,  ...,  0.0140,  0.1083, -0.2083],\n",
      "        [-0.8854,  1.5881,  0.6256,  ...,  2.1531, -0.4263, -1.3524],\n",
      "        [ 0.3115, -0.1987,  0.7019,  ...,  0.1874, -0.4023, -0.4572],\n",
      "        [ 1.5598,  0.0419,  0.5987,  ...,  0.6928,  0.5698,  0.2840]])\n",
      "\n",
      "Out[7]: torch.Size([4, 1024])\n",
      "\n",
      "Out[8]: torch.Size([4, 1024])\n",
      "\n",
      "Out[9]: \n",
      "tensor([[-0.0871, -0.2618, -1.1075,  ...,  0.0140,  0.1083, -0.2083],\n",
      "        [-0.8854,  1.5881,  0.6256,  ...,  2.1531, -0.4263, -1.3524],\n",
      "        [ 0.3115, -0.1987,  0.7019,  ...,  0.1874, -0.4023, -0.4572],\n",
      "        [ 1.5598,  0.0419,  0.5987,  ...,  0.6928,  0.5698,  0.2840]])\n",
      "\n",
      "Out[10]: [0, 1, 2, 3, 3, 3, 3]\n",
      "\n",
      "Out[11]: \n",
      "tensor([[-0.0871, -0.2618, -1.1075,  ...,  0.0140,  0.1083, -0.2083],\n",
      "        [-0.8854,  1.5881,  0.6256,  ...,  2.1531, -0.4263, -1.3524],\n",
      "        [ 0.3115, -0.1987,  0.7019,  ...,  0.1874, -0.4023, -0.4572],\n",
      "        ...,\n",
      "        [ 1.5598,  0.0419,  0.5987,  ...,  0.6928,  0.5698,  0.2840],\n",
      "        [ 1.5598,  0.0419,  0.5987,  ...,  0.6928,  0.5698,  0.2840],\n",
      "        [ 1.5598,  0.0419,  0.5987,  ...,  0.6928,  0.5698,  0.2840]])\n",
      "\n",
      "Out[12]: torch.Size([7, 1024])\n",
      "\n",
      "Out[13]: torch.Size([7, 27])\n",
      "\n",
      "Out[14]: \n",
      "tensor([[-3.5072, -3.8106, -3.7676, -3.1425, -3.1325, -2.9407, -3.3746, -3.7482,\n",
      "         -2.8571, -2.9943, -3.1338, -3.1611, -3.6895, -2.3389, -4.0384, -4.0982,\n",
      "         -3.0354, -2.8496, -2.9375, -4.0170, -2.6357, -3.5220, -4.7241, -3.8044,\n",
      "         -3.6820, -3.9952, -3.6718],\n",
      "        [-3.9674, -3.0713, -2.9894, -3.1793, -3.5584, -3.9955, -3.8362, -4.3184,\n",
      "         -2.8129, -2.7096, -2.9789, -3.0267, -2.8278, -2.3906, -3.2680, -3.6584,\n",
      "         -3.3421, -3.5171, -3.4283, -3.7368, -3.5274, -2.8984, -4.6644, -3.7654,\n",
      "         -3.7290, -3.3855, -3.7196],\n",
      "        [-3.0171, -3.4490, -3.9804, -2.8042, -3.7291, -3.6980, -3.3556, -4.5430,\n",
      "         -2.8565, -2.9660, -3.3979, -3.7966, -3.5578, -2.4629, -2.7176, -3.6748,\n",
      "         -2.5003, -2.8531, -4.2323, -3.8925, -2.8460, -3.2389, -4.6181, -3.8110,\n",
      "         -3.9864, -3.8383, -3.4493],\n",
      "        [-3.5983, -4.3701, -3.6901, -2.4458, -3.3408, -3.2872, -3.8510, -4.4979,\n",
      "         -2.5389, -3.1500, -3.2810, -2.8068, -3.3163, -2.2012, -3.2987, -3.9757,\n",
      "         -2.8204, -3.4415, -4.4454, -3.5773, -3.1376, -2.9519, -4.8541, -4.2695,\n",
      "         -3.9353, -3.2130, -4.4000],\n",
      "        [-3.2984, -4.7339, -2.9620, -2.7572, -3.4304, -3.4639, -2.8731, -4.4120,\n",
      "         -2.6529, -2.6141, -4.3297, -3.4668, -3.3836, -2.2453, -3.3379, -3.6268,\n",
      "         -3.8289, -3.9017, -3.2165, -3.7121, -2.8159, -3.1885, -4.0184, -3.8885,\n",
      "         -3.5325, -4.3074, -3.5745],\n",
      "        [-3.3194, -4.3981, -3.7465, -2.9357, -3.0585, -3.4406, -3.7778, -4.5375,\n",
      "         -2.9718, -3.4600, -3.7325, -3.1150, -3.9376, -2.8244, -3.6562, -3.4875,\n",
      "         -2.6586, -3.7158, -3.6208, -3.8193, -2.9470, -2.0020, -4.1114, -4.2698,\n",
      "         -3.1045, -3.7475, -3.0656],\n",
      "        [-4.0109, -3.8315, -3.0818, -2.7791, -3.3676, -3.0587, -3.6011, -4.0460,\n",
      "         -3.2103, -3.3565, -3.5691, -3.4102, -3.2553, -2.4898, -3.9544, -3.7940,\n",
      "         -2.9431, -3.6221, -3.5451, -3.8631, -3.1086, -2.2922, -4.2665, -3.7966,\n",
      "         -3.0617, -3.3049, -3.4595]], grad_fn=<LogSoftmaxBackward0>)\n",
      "\n",
      "Out[15]: torch.Size([7, 27])\n",
      "\n",
      "Out[16]: [0, 0, 0, tensor(3), tensor(1), tensor(20), 0]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/var/folders/23/hpkxkv9121b6yf_cr4hmvw900000gn/T/ipykernel_80151/3145532236.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m alignment_probs \u001b[39m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m all_distinct_permutations:\n\u001b[0;32m---> 22\u001b[0m     alignment_prob \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mcompute_single_alignment_prob(encoder_out[\u001b[39m0\u001b[39;49m], predictor_out[\u001b[39m0\u001b[39;49m], T\u001b[39m.\u001b[39;49mitem(), U\u001b[39m.\u001b[39;49mitem(), z, y[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     23\u001b[0m     alignment_probs\u001b[39m.\u001b[39mappend(alignment_prob)\n\u001b[1;32m     24\u001b[0m loss_enumerate \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mtensor(alignment_probs)\u001b[39m.\u001b[39mlogsumexp(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/var/folders/23/hpkxkv9121b6yf_cr4hmvw900000gn/T/ipykernel_80151/3186918183.py:100\u001b[0m, in \u001b[0;36mTransducer.compute_single_alignment_prob\u001b[0;34m(self, encoder_out, predictor_out, T, U, z, y)\u001b[0m\n\u001b[1;32m     98\u001b[0m joiner_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoiner\u001b[39m.\u001b[39mforward(encoder_out_expanded, predictor_out_expanded)\u001b[39m.\u001b[39mlog_softmax(\u001b[39m1\u001b[39m)\n\u001b[1;32m     99\u001b[0m logprob \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnll_loss(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mjoiner_out, target\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor(y_expanded)\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m IPython\u001b[39m.\u001b[39;49membed()\n\u001b[1;32m    101\u001b[0m \u001b[39mreturn\u001b[39;00m logprob\n",
      "File \u001b[0;32m~/Desktop/wip/venv/lib/python3.8/site-packages/IPython/terminal/embed.py:399\u001b[0m, in \u001b[0;36membed\u001b[0;34m(header, compile_flags, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m frame \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe(\u001b[39m1\u001b[39m)\n\u001b[1;32m    397\u001b[0m shell \u001b[39m=\u001b[39m InteractiveShellEmbed\u001b[39m.\u001b[39minstance(_init_location_id\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[1;32m    398\u001b[0m     frame\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_filename, frame\u001b[39m.\u001b[39mf_lineno), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 399\u001b[0m shell(header\u001b[39m=\u001b[39;49mheader, stack_depth\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, compile_flags\u001b[39m=\u001b[39;49mcompile_flags,\n\u001b[1;32m    400\u001b[0m     _call_location_id\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m (frame\u001b[39m.\u001b[39;49mf_code\u001b[39m.\u001b[39;49mco_filename, frame\u001b[39m.\u001b[39;49mf_lineno))\n\u001b[1;32m    401\u001b[0m InteractiveShellEmbed\u001b[39m.\u001b[39mclear_instance()\n\u001b[1;32m    402\u001b[0m \u001b[39m#restore previous instance\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/wip/venv/lib/python3.8/site-packages/IPython/terminal/embed.py:245\u001b[0m, in \u001b[0;36mInteractiveShellEmbed.__call__\u001b[0;34m(self, header, local_ns, module, dummy, stack_depth, compile_flags, **kw)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshow_banner()\n\u001b[1;32m    243\u001b[0m \u001b[39m# Call the embedding code with a stack depth of 1 so it can skip over\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m# our call and get the original caller's namespaces.\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmainloop(\n\u001b[1;32m    246\u001b[0m     local_ns, module, stack_depth\u001b[39m=\u001b[39;49mstack_depth, compile_flags\u001b[39m=\u001b[39;49mcompile_flags\n\u001b[1;32m    247\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbanner2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mold_banner2\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexit_msg \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/wip/venv/lib/python3.8/site-packages/IPython/terminal/embed.py:337\u001b[0m, in \u001b[0;36mInteractiveShellEmbed.mainloop\u001b[0;34m(self, local_ns, module, stack_depth, compile_flags)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_completer_frame()\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplay_trap:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minteract()\n\u001b[1;32m    339\u001b[0m \u001b[39m# now, purge out the local namespace of IPython's hidden variables.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m local_ns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/wip/venv/lib/python3.8/site-packages/IPython/terminal/interactiveshell.py:670\u001b[0m, in \u001b[0;36mTerminalInteractiveShell.interact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseparate_in, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt_for_code()\n\u001b[1;32m    671\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEOFError\u001b[39;00m:\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfirm_exit) \\\n\u001b[1;32m    673\u001b[0m             \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mask_yes_no(\u001b[39m'\u001b[39m\u001b[39mDo you really want to exit ([y]/n)?\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/wip/venv/lib/python3.8/site-packages/IPython/terminal/interactiveshell.py:429\u001b[0m, in \u001b[0;36mTerminalInteractiveShell.init_prompt_toolkit_cli.<locals>.prompt\u001b[0;34m()\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprompt\u001b[39m():\n\u001b[1;32m    428\u001b[0m     prompt_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompts\u001b[39m.\u001b[39min_prompt_tokens())\n\u001b[0;32m--> 429\u001b[0m     lines \u001b[39m=\u001b[39m [\u001b[39minput\u001b[39;49m(prompt_text)]\n\u001b[1;32m    430\u001b[0m     prompt_continuation \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompts\u001b[39m.\u001b[39mcontinuation_prompt_tokens())\n\u001b[1;32m    431\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_complete(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(lines))[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mincomplete\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/wip/venv/lib/python3.8/site-packages/IPython/utils/py3compat.py:48\u001b[0m, in \u001b[0;36minput\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minput\u001b[39m(prompt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m builtin_mod\u001b[39m.\u001b[39;49minput(prompt)\n",
      "File \u001b[0;32m~/Desktop/wip/venv/lib/python3.8/site-packages/ipykernel/kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[1;32m   1192\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[1;32m   1193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1194\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1195\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1196\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/wip/venv/lib/python3.8/site-packages/ipykernel/kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   1235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate example inputs/outputs\n",
    "num_outputs = len(string.ascii_uppercase) + 1 # [null, A, B, ... Z]\n",
    "model = Transducer(1, num_outputs)\n",
    "y_letters = \"CAT\"\n",
    "y = torch.tensor([string.ascii_uppercase.index(l) + 1 for l in y_letters]).unsqueeze(0).to(model.device) # tokenize\n",
    "T = torch.tensor([4]) # Time\n",
    "U = torch.tensor([len(y_letters)]) # Number of labels\n",
    "B = 1 # Batch\n",
    "\n",
    "# encoder_out and predictor_out have the same dimentions.\n",
    "encoder_out = torch.randn(B, T, joiner_dim).to(model.device) # Fairly sure T = 4 because of NULL_INDEX (0) token prepended to the input. TODO: confirm this.\n",
    "predictor_out = torch.randn(B, U+1, joiner_dim).to(model.device) # Fairly sure U = 3 + 1 because NULL_INDEX (0) token prepended to the input. TODO: confirm this.\n",
    "joiner_out = model.joiner.forward(encoder_out.unsqueeze(2), predictor_out.unsqueeze(1)).log_softmax(3)\n",
    "\n",
    "#######################################################\n",
    "# Compute loss by enumerating all possible alignments #\n",
    "#######################################################\n",
    "all_permutations = list(itertools.permutations([0]*(T-1) + [1]*U))\n",
    "all_distinct_permutations = list(Counter(all_permutations).keys())\n",
    "alignment_probs = []\n",
    "for z in all_distinct_permutations:\n",
    "    alignment_prob = model.compute_single_alignment_prob(encoder_out[0], predictor_out[0], T.item(), U.item(), z, y[0])\n",
    "    alignment_probs.append(alignment_prob)\n",
    "loss_enumerate = -torch.tensor(alignment_probs).logsumexp(0)\n",
    "IPython.embed()\n",
    "#######################################################\n",
    "# Compute loss using the forward algorithm            #\n",
    "#######################################################\n",
    "loss_forward = -model.compute_forward_prob(joiner_out, T, U, y)\n",
    "\n",
    "print(\"Loss computed by enumerating all possible alignments: \", loss_enumerate)\n",
    "print(\"Loss computed using the forward algorithm: \", loss_forward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
