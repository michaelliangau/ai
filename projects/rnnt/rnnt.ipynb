{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transducer PyTorch implementation\n",
    "Goal: In this notebook, we'll use it to insert missing vowels into a sentence.\n",
    "\n",
    "This is an image of the architecture.\n",
    "\n",
    "![RNN-T architecture](images/rnnt_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "from collections import Counter\n",
    "# from speechbrain.nnet.loss.transducer_loss import TransducerLoss\n",
    "import unidecode\n",
    "import IPython\n",
    "import string\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "# Get training data\n",
    "file_path = \"war_and_peace.txt\"\n",
    "url = \"https://raw.githubusercontent.com/lorenlugosch/infer_missing_vowels/master/data/train/war_and_peace.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Downloading file...\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    print(\"File downloaded.\")\n",
    "else:\n",
    "    print(\"File already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "NULL_INDEX = 0\n",
    "encoder_dim = 1024\n",
    "predictor_dim = 1024\n",
    "joiner_dim = 1024\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder network\n",
    "# The encoder is any network that can take as input a variable-length sequence: so, RNNs, CNNs, and self-attention/Transformer encoders will all work.\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder network.\n",
    "    \n",
    "    Input: Audio input\n",
    "\n",
    "    Output: Encoded speech features. Context-aware encodings.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(num_inputs, encoder_dim)\n",
    "        self.rnn = torch.nn.GRU(input_size=encoder_dim, hidden_size=encoder_dim, num_layers=3, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.linear = torch.nn.Linear(encoder_dim*2, joiner_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.embed(out)\n",
    "        out = self.rnn(out)[0]\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor network\n",
    "# The predictor is any causal network (= can't look at the future): in other words, unidirectional RNNs, causal convolutions, or masked self-attention.\n",
    "\n",
    "class Predictor(torch.nn.Module):\n",
    "    \"\"\"Predictor network.\n",
    "    \n",
    "    Input: Text inputs (labels).\n",
    "    \n",
    "    Output: RNN hidden states for each autoregressive timestep input.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_outputs):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(num_outputs, predictor_dim)\n",
    "        self.rnn = torch.nn.GRUCell(input_size=predictor_dim, hidden_size=predictor_dim)\n",
    "        self.linear = torch.nn.Linear(predictor_dim, joiner_dim)\n",
    "        \n",
    "        self.initial_state = torch.nn.Parameter(torch.randn(predictor_dim))\n",
    "        self.start_symbol = NULL_INDEX # In the original paper, a vector of 0s is used; just using the null index instead is easier when using an Embedding layer.\n",
    "\n",
    "    def forward_one_step(self, input, previous_state):\n",
    "        embedding = self.embed(input)\n",
    "        state = self.rnn.forward(embedding, previous_state)\n",
    "        out = self.linear(state)\n",
    "        return out, state\n",
    "\n",
    "    def forward(self, y):\n",
    "        batch_size = y.shape[0]\n",
    "        U = y.shape[1]\n",
    "        outs = []\n",
    "        state = torch.stack([self.initial_state] * batch_size).to(y.device)\n",
    "        for u in range(U+1): # need U+1 to get null output for final timestep \n",
    "            if u == 0:\n",
    "                decoder_input = torch.tensor([self.start_symbol] * batch_size).to(y.device)\n",
    "            else:\n",
    "                decoder_input = y[:,u-1]\n",
    "            out, state = self.forward_one_step(decoder_input, state)\n",
    "            outs.append(out)\n",
    "        out = torch.stack(outs, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joiner(torch.nn.Module):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(Joiner, self).__init__()\n",
    "        self.linear = torch.nn.Linear(joiner_dim, num_outputs)\n",
    "\n",
    "    def forward(self, encoder_out, predictor_out):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            encoder_out: Output of the encoder network. Shape [batch_size, T, 1, encoder_dim]. # TODO Confirm that these shapes are correct.\n",
    "            predictor_out: Output of the predictor network. Shape [batch_size, 1, U, predictor_dim]. # TODO Confirm that these shapes are correct.\n",
    "        \"\"\"\n",
    "        out = encoder_out + predictor_out\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alignment matrix\n",
    "\n",
    "![RNN-T alignment matrix](images/rnnt_alignment_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transducer(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Transducer, self).__init__()\n",
    "        self.encoder = Encoder(num_inputs)\n",
    "        self.predictor = Predictor(num_outputs)\n",
    "        self.joiner = Joiner(num_outputs)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda:0\"\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def compute_forward_prob(self, joiner_out, T, U, y):\n",
    "        \"\"\"Compute forward probability.\n",
    "\n",
    "        Args:\n",
    "            joiner_out: tensor of shape (B, T_max, U_max+1, #labels)\n",
    "            T: list of input lengths\n",
    "            U: list of output lengths \n",
    "            y: label tensor (B, U_max+1)\n",
    "\n",
    "        Returns:\n",
    "            log_probs: log probs of each possible alignment path between input and output. tensor of shape (B)\n",
    "        \"\"\"\n",
    "        B = joiner_out.shape[0]\n",
    "        T_max = joiner_out.shape[1]\n",
    "        U_max = joiner_out.shape[2] - 1\n",
    "        log_alpha = torch.zeros(B, T_max, U_max+1).to(self.device)\n",
    "        for t in range(T_max):\n",
    "            for u in range(U_max+1):\n",
    "                # The log_alpha tensor stores the summed forward probabilities up to a certain timestep, considering all possible paths to get to the timestep (null index = right/increment t or label index = down/increment u).\n",
    "                # The following explains each case covered:\n",
    "                # Case (t == 0) and (u == 0): This is the initial state. The forward probability is set to 0, representing a probability of 1 in log space.\n",
    "                # Case (t > 0) and (u == 0): This case calculates the forward probability for the beginning of the output sequence by adding the previous forward probability (log_alpha[:, t-1, u]) with the joiner output probability for the NULL_INDEX.\n",
    "                # Case (t == 0) and (u > 0): This case calculates the forward probability for the beginning of the input sequence. It does this by adding the previous forward probability (log_alpha[:, t, u-1]) with the joiner output probability at the corresponding label (y[:, u-1]).\n",
    "                # Case (t > 0) and (u > 0): This case calculates the forward probability for other (t, u) pairs in the lattice. It computes the log sum of exponentials of the two possible paths:\n",
    "                    #a) from the state (t-1, u) with the joiner output probability for the NULL_INDEX,\n",
    "                    # b) from the state (t, u-1) with the joiner output probability at the corresponding label (y[:, u-1]).\n",
    "                if u == 0:\n",
    "                    if t == 0:\n",
    "                        log_alpha[:, t, u] = 0.\n",
    "\n",
    "                    else: #t > 0\n",
    "                        log_alpha[:, t, u] = log_alpha[:, t-1, u] + joiner_out[:, t-1, 0, NULL_INDEX] \n",
    "                            \n",
    "                else: #u > 0\n",
    "                    if t == 0:\n",
    "                        log_alpha[:, t, u] = log_alpha[:, t,u-1] + torch.gather(joiner_out[:, t, u-1], dim=1, index=y[:,u-1].view(-1,1) ).reshape(-1)\n",
    "                    \n",
    "                    else: #t > 0\n",
    "                        log_alpha[:, t, u] = torch.logsumexp(torch.stack([\n",
    "                            log_alpha[:, t-1, u] + joiner_out[:, t-1, u, NULL_INDEX],\n",
    "                            log_alpha[:, t, u-1] + torch.gather(joiner_out[:, t, u-1], dim=1, index=y[:,u-1].view(-1,1) ).reshape(-1)\n",
    "                        ]), dim=0)\n",
    "\n",
    "\n",
    "        log_probs = []\n",
    "        for b in range(B):\n",
    "            log_prob = log_alpha[b, T[b]-1, U[b]] + joiner_out[b, T[b]-1, U[b], NULL_INDEX] # Add forward probability of final step (1, 3, 3 = which is a cumulated prob) and the null probability.\n",
    "            log_probs.append(log_prob)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        return log_prob\n",
    "\n",
    "    def compute_loss(self, x, y, T, U):\n",
    "        encoder_out = self.encoder.forward(x)\n",
    "        predictor_out = self.predictor.forward(y)\n",
    "        joiner_out = self.joiner.forward(encoder_out.unsqueeze(2), predictor_out.unsqueeze(1)).log_softmax(3)\n",
    "        loss = -self.compute_forward_prob(joiner_out, T, U, y).mean()\n",
    "        return loss\n",
    "  \n",
    "    def compute_single_alignment_prob(self, encoder_out, predictor_out, T, U, z, y):\n",
    "        \"\"\"Computes the probability of one alignment, z.\n",
    "\n",
    "        What does the encoder_out network represent?\n",
    "        What does the predictor_out network represent?\n",
    "\n",
    "\n",
    "        Args:\n",
    "            encoder_out: Output of the encoder network. Shape [T, encoder_dim]. T = 4. In this example it is [4, 1024]\n",
    "            predictor_out: Output of the predictor network. Shape [U, predictor_dim]. U = 3+1 = 4. In this example it is [4, 1024]\n",
    "            T: Length of the encoder_out sequence.\n",
    "            U: Length of the predictor_out sequence.\n",
    "            z: Alignment. List of 0s and 1s, where 0 means \"right\" (increment t, encoder output) and 1 means \"down\" (increment u, label).\n",
    "            y: Label sequence. List of labels.\n",
    "        \n",
    "        Returns:\n",
    "            logprob: Log probability of the alignment.\n",
    "        \"\"\"\n",
    "        t = 0; u = 0 # t and u are merely movements in the alignment matrix that we'll use to index into the encoder_out and predictor_out matrices.\n",
    "        t_u_indices = []\n",
    "        y_expanded = []\n",
    "        for step in z:\n",
    "            t_u_indices.append((t,u))\n",
    "            if step == 0: # right (null)\n",
    "                y_expanded.append(NULL_INDEX)\n",
    "                t += 1\n",
    "            if step == 1: # down (label)\n",
    "                y_expanded.append(y[u])\n",
    "                u += 1\n",
    "\n",
    "        t_u_indices.append((T-1,U))\n",
    "        y_expanded.append(NULL_INDEX)\n",
    "\n",
    "        t_indices = [t for (t,u) in t_u_indices]\n",
    "        u_indices = [u for (t,u) in t_u_indices]\n",
    "        encoder_out_expanded = encoder_out[t_indices] # Indexed into the encoder/predictor_out lists using the t_u alignments.\n",
    "        predictor_out_expanded = predictor_out[u_indices]\n",
    "        joiner_out = self.joiner.forward(encoder_out_expanded, predictor_out_expanded).log_softmax(1) # joiner network takes the alignments at each timestep ((0, 0) hidden states) as inputs through it and pops out softmaxed probabilities.\n",
    "        logprob = -torch.nn.functional.nll_loss(input=joiner_out, target=torch.tensor(y_expanded).long().to(self.device), reduction=\"sum\") # For each permutation of the alignment matrix (encoder_out + predictor_out for each step), expect the target outcome is the z alignment sequence (0 or tensor(3)).\n",
    "        return logprob  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss computed by enumerating all possible alignments:  tensor(24.2029)\n",
      "Loss computed using the forward algorithm:  tensor(24.2029, grad_fn=<NegBackward0>)\n",
      "Losses are equal.\n"
     ]
    }
   ],
   "source": [
    "# Generate example inputs/outputs\n",
    "num_outputs = len(string.ascii_uppercase) + 1 # [null, A, B, ... Z]\n",
    "model = Transducer(1, num_outputs)\n",
    "y_letters = \"CAT\"\n",
    "y = torch.tensor([string.ascii_uppercase.index(l) + 1 for l in y_letters]).unsqueeze(0).to(model.device) # tokenize\n",
    "T = torch.tensor([4]) # Time\n",
    "U = torch.tensor([len(y_letters)]) # Number of labels\n",
    "B = 1 # Batch\n",
    "\n",
    "# encoder_out and predictor_out have the same dimentions.\n",
    "encoder_out = torch.randn(B, T, joiner_dim).to(model.device) # Fairly sure T = 4 because of NULL_INDEX (0) token prepended to the input. TODO: confirm this.\n",
    "predictor_out = torch.randn(B, U+1, joiner_dim).to(model.device) # Fairly sure U = 3 + 1 because NULL_INDEX (0) token prepended to the input. TODO: confirm this.\n",
    "joiner_out = model.joiner.forward(encoder_out.unsqueeze(2), predictor_out.unsqueeze(1)).log_softmax(3)\n",
    "\n",
    "#######################################################\n",
    "# Compute loss by enumer/ating all possible alignments #\n",
    "#######################################################\n",
    "all_permutations = list(itertools.permutations([0]*(T-1) + [1]*U))\n",
    "all_distinct_permutations = list(Counter(all_permutations).keys())\n",
    "alignment_probs = []\n",
    "for z in all_distinct_permutations:\n",
    "    alignment_prob = model.compute_single_alignment_prob(encoder_out[0], predictor_out[0], T.item(), U.item(), z, y[0])\n",
    "    alignment_probs.append(alignment_prob)\n",
    "loss_enumerate = -torch.tensor(alignment_probs).logsumexp(0) # log of the sum of the exponentials of all the alignment probability losses.\n",
    "\n",
    "#######################################################\n",
    "# Compute loss using the forward algorithm            #\n",
    "#######################################################\n",
    "loss_forward = -model.compute_forward_prob(joiner_out, T, U, y) # the probability of seeing the output given all possible alignments suggested by joiner_out outputs.\n",
    "\n",
    "print(\"Loss computed by enumerating all possible alignments: \", loss_enumerate)\n",
    "print(\"Loss computed using the forward algorithm: \", loss_forward)\n",
    "if torch.allclose(loss_enumerate, loss_forward):\n",
    "    print(\"Losses are equal.\") # Losses should be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(self, x, T):\n",
    "  \"\"\"Greey search algorithm used to predict an output sequence.\"\"\"\n",
    "  # TODO understand this\n",
    "  y_batch = []\n",
    "  B = len(x)\n",
    "  encoder_out = self.encoder.forward(x)\n",
    "  U_max = 200\n",
    "  for b in range(B):\n",
    "    t = 0; u = 0; y = [self.predictor.start_symbol]; predictor_state = self.predictor.initial_state.unsqueeze(0)\n",
    "    while t < T[b] and u < U_max:\n",
    "      predictor_input = torch.tensor([ y[-1] ]).to(x.device)\n",
    "      g_u, predictor_state = self.predictor.forward_one_step(predictor_input, predictor_state)\n",
    "      f_t = encoder_out[b, t]\n",
    "      h_t_u = self.joiner.forward(f_t, g_u)\n",
    "      argmax = h_t_u.max(-1)[1].item()\n",
    "      if argmax == NULL_INDEX:\n",
    "        t += 1\n",
    "      else: # argmax == a label\n",
    "        u += 1\n",
    "        y.append(argmax)\n",
    "    y_batch.append(y[1:]) # remove start symbol\n",
    "  return y_batch\n",
    "\n",
    "Transducer.greedy_search = greedy_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a faster version of the transudcer loss from speechbrain library. Presumably because it's not written in pure Python.\n",
    "# TODO This doesn't work on non-CUDA devices.\n",
    "\n",
    "# transducer_loss = TransducerLoss(0)\n",
    "\n",
    "# def compute_loss(self, x, y, T, U):\n",
    "#     encoder_out = self.encoder.forward(x)\n",
    "#     predictor_out = self.predictor.forward(y)\n",
    "#     joiner_out = self.joiner.forward(encoder_out.unsqueeze(2), predictor_out.unsqueeze(1)).log_softmax(3)\n",
    "#     #loss = -self.compute_forward_prob(joiner_out, T, U, y).mean()\n",
    "#     T = T.to(joiner_out.device)\n",
    "#     U = U.to(joiner_out.device)\n",
    "#     loss = transducer_loss(joiner_out, y, T, U) #, blank_index=NULL_INDEX, reduction=\"mean\")\n",
    "#     return loss\n",
    "\n",
    "# Transducer.compute_loss = compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"Wll, Prnc, s Gn nd Lcc r nw jst fmly stts f th',\n",
       " '\"Well, Prince, so Genoa and Lucca are now just family estates of the')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "  \"\"\"Creates text dataset with vowels removed from input.\"\"\"\n",
    "  def __init__(self, lines, batch_size):\n",
    "    lines = list(filter((\"\\n\").__ne__, lines))\n",
    "\n",
    "    self.lines = lines # list of strings\n",
    "    collate = Collate()\n",
    "    self.loader = torch.utils.data.DataLoader(self, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.lines)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    line = self.lines[idx].replace(\"\\n\", \"\")\n",
    "    line = unidecode.unidecode(line) # remove special characters\n",
    "    x = \"\".join(c for c in line if c not in \"AEIOUaeiou\") # remove vowels from input\n",
    "    y = line\n",
    "    return (x,y)\n",
    "\n",
    "def encode_string(s):\n",
    "  for c in s:\n",
    "    if c not in string.printable:\n",
    "      print(s)\n",
    "  return [string.printable.index(c) + 1 for c in s]\n",
    "\n",
    "def decode_labels(l):\n",
    "  return \"\".join([string.printable[c - 1] for c in l])\n",
    "\n",
    "class Collate:\n",
    "  def __call__(self, batch):\n",
    "    \"\"\"\n",
    "    batch: list of tuples (input string, output string)\n",
    "    Returns a minibatch of strings, encoded as labels and padded to have the same length.\n",
    "    \"\"\"\n",
    "    x = []; y = []\n",
    "    batch_size = len(batch)\n",
    "    for index in range(batch_size):\n",
    "      x_,y_ = batch[index]\n",
    "      x.append(encode_string(x_))\n",
    "      y.append(encode_string(y_))\n",
    "\n",
    "    # pad all sequences to have same length\n",
    "    T = [len(x_) for x_ in x]\n",
    "    U = [len(y_) for y_ in y]\n",
    "    T_max = max(T)\n",
    "    U_max = max(U)\n",
    "    for index in range(batch_size):\n",
    "      x[index] += [NULL_INDEX] * (T_max - len(x[index]))\n",
    "      x[index] = torch.tensor(x[index])\n",
    "      y[index] += [NULL_INDEX] * (U_max - len(y[index]))\n",
    "      y[index] = torch.tensor(y[index])\n",
    "\n",
    "    # stack into single tensor\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    T = torch.tensor(T)\n",
    "    U = torch.tensor(U)\n",
    "\n",
    "    return (x,y,T,U)\n",
    "\n",
    "with open(\"war_and_peace.txt\", \"r\") as f:\n",
    "  lines = f.readlines()\n",
    "\n",
    "end = round(0.9 * len(lines))\n",
    "train_lines = lines[:end]\n",
    "test_lines = lines[end:]\n",
    "train_set = TextDataset(train_lines, batch_size=64) #8)\n",
    "test_set = TextDataset(test_lines, batch_size=64) #8)\n",
    "train_set.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__(self, model, lr):\n",
    "    self.model = model\n",
    "    self.lr = lr\n",
    "    self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
    "  \n",
    "  def train(self, dataset, print_interval = 20):\n",
    "      train_loss = 0\n",
    "      num_samples = 0\n",
    "      self.model.train()\n",
    "      pbar = tqdm(dataset.loader)\n",
    "      for idx, batch in enumerate(pbar):\n",
    "        x,y,T,U = batch\n",
    "        x = x.to(self.model.device); y = y.to(self.model.device)\n",
    "        batch_size = len(x)\n",
    "        num_samples += batch_size\n",
    "        loss = self.model.compute_loss(x,y,T,U)\n",
    "        self.optimizer.zero_grad()\n",
    "        pbar.set_description(\"%.2f\" % loss.item())\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        train_loss += loss.item() * batch_size\n",
    "        if idx % print_interval == 0:\n",
    "          self.model.eval()\n",
    "          guesses = self.model.greedy_search(x,T)\n",
    "          self.model.train()\n",
    "          print(\"\\n\")\n",
    "          for b in range(2):\n",
    "            print(\"input:\", decode_labels(x[b,:T[b]]))\n",
    "            print(\"guess:\", decode_labels(guesses[b]))\n",
    "            print(\"truth:\", decode_labels(y[b,:U[b]]))\n",
    "            print(\"\")\n",
    "\n",
    "      train_loss /= num_samples\n",
    "      return train_loss\n",
    "\n",
    "  def test(self, dataset, print_interval=1):\n",
    "    test_loss = 0\n",
    "    num_samples = 0\n",
    "    self.model.eval()\n",
    "    pbar = tqdm(dataset.loader)\n",
    "    for idx, batch in enumerate(pbar):\n",
    "      x,y,T,U = batch\n",
    "      x = x.to(self.model.device); y = y.to(self.model.device)\n",
    "      batch_size = len(x)\n",
    "      num_samples += batch_size\n",
    "      loss = self.model.compute_loss(x,y,T,U)\n",
    "      pbar.set_description(\"%.2f\" % loss.item())\n",
    "      test_loss += loss.item() * batch_size\n",
    "      if idx % print_interval == 0:\n",
    "        print(\"\\n\")\n",
    "        print(\"input:\", decode_labels(x[0,:T[0]]))\n",
    "        print(\"guess:\", decode_labels(self.model.greedy_search(x,T)[0]))\n",
    "        print(\"truth:\", decode_labels(y[0,:U[0]]))\n",
    "        print(\"\")\n",
    "    test_loss /= num_samples\n",
    "    return test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "476.93:   0%|          | 0/709 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "num_chars = len(string.printable)\n",
    "model = Transducer(num_inputs=num_chars+1, num_outputs=num_chars+1)\n",
    "trainer = Trainer(model=model, lr=0.0003)\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = trainer.train(train_set)\n",
    "    test_loss = trainer.test(test_set)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    print(\"Epoch %d: train loss = %f, test loss = %f\" % (epoch, train_loss, test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
